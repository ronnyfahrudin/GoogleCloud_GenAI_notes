{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Explaining text classification with Vertex Explainable AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "Machine learning models are complex systems, and developers require advanced tools to understand and explain (some of) their behavior. Vertex AI, Google Cloud's platform for end-to-end ML, has a specialized offering for model interpretability called Explainable AI. \n",
    "\n",
    "[Vertex Explainable AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview) provides two different explanation types to better understand model decision making:\n",
    "- *Feature-based explanations* indicate how much each feature in the model contributed to the predictions for a chosen input instance. \n",
    "- *Example-based explanations* provide a list of examples (typically from the training set) that are most similar to the chosen input instance. \n",
    "\n",
    "Vertex Explainable AI can be used with a custom-trained model, an AutoML model, or a BigQueryML model. \n",
    "\n",
    "**Note:** *you can either select feature-based or example-based explanations for one model.*\n",
    "\n",
    "\n",
    "<hr style=\"border: 0.3px solid grey; width:40%;\"></hr>\n",
    "\n",
    "\n",
    "In this lab, you use Vertex Explainable AI to get feature-based explanations for a custom-trained model for a sentiment analysis task built on the [IMDB dataset](https://keras.io/api/datasets/imdb/). Given your custom-trained model, you need to:\n",
    "1. Upload the model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction) with the desired explanation specs.\n",
    "\n",
    "2. Deploy the model as a [Vertex AI Endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment).\n",
    "\n",
    "3. Send prediction requests to the model endpoint, and analyze your explanation results!\n",
    "\n",
    "**Note:** *this lab extends the publicly-available notebook [\"Explaining text classification with Vertex Explainable AI\"](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/xai_text_classification_feature_attributions.ipynb) hosted in the Vertex AI Samples git repo.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this lab, you will learn how to perform some model interpretability by:\n",
    "\n",
    "- Using Vertex AI Model Registry to load a custom-trained model and configure explanations\n",
    "- Using Vertex AI Explainable AI to get feature-based explanations with the *sampled Shapley method*.\n",
    "- Visualizing explanations with matplotlib, and gathering some model insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f",
    "tags": []
   },
   "source": [
    "## Set-up\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "1. import the necessary dependencies for the libraries you'll be using in this exercise, which include [google-cloud-aiplatform](https://pypi.org/project/google-cloud-aiplatform/) and [Tensorflow Keras](https://www.tensorflow.org/guide/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform, storage\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize *aiplatform* with a baseline configuration for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "# These variables define your google project information\n",
    "# The BUCKET_URI specifies the name of the bucket you'll be using, unique to your project\n",
    "project_id_list = !gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID = project_id_list[0]\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = f\"gs://explainable-ai-{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwiklabs-gcp-01-e97fc0ced76b']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BucketNotFoundException: 404 gs://explainable-ai-qwiklabs-gcp-01-e97fc0ced76b bucket does not exist.\n",
      "Creating gs://explainable-ai-qwiklabs-gcp-01-e97fc0ced76b/...\n"
     ]
    }
   ],
   "source": [
    "# Only if your bucket doesn't already exist: run the following command to create your Cloud Storage bucket.\n",
    "! gsutil ls $BUCKET_URI > /dev/null || gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "utUNuq2aARaE"
   },
   "outputs": [],
   "source": [
    "# Initialize the Vertex AI SDK for Python for your project.\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the IMDB dataset\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [IMDB Reviews dataset](https://keras.io/api/datasets/imdb/) is a set of 25,000 movie reviews that have been labeled as either positive or negative. The text of the reviews has been tokenized according to frequency of appearance in the corpus. Frequencies are offset from a zero-base by 3 for reserved tokens (0, 1, and 2). Token \"3\" represents the most frequently seen word. Our model is going to take these numeric representations of the text and try to learn contextual meaning from them based on how they are used in both positive and negative reviews.\n",
    "\n",
    "To keep the model simple for this example, you are only going to take the 10,000 most commonly seen words across all reviews. Importantly, the tokens represent exact words. The singular \"word\" and plural \"words\" would be two different tokens (678 and 712, respectively). You will also take the final 80 words of the review, or pad the beginning of the review to 80 words if it is shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "poetFd8bwHFb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "maxlen = 80\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display a review to see the data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,\n",
       "        530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,\n",
       "        104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,\n",
       "        141,    6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,\n",
       "         26,  480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,\n",
       "         92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,\n",
       "         16,  283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,\n",
       "         19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 80 integer tokens representing the 80 final words of the review. The token \"0\" is the padding token. If you had seen that at the beginning, it would have meant the review was shorter than 80 words. The token \"2\" represents a word that is outside the 10,000-most-common-word vocabulary you established. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqI_MEPvWVI5",
    "tags": []
   },
   "source": [
    "## Build a TensorFlow model locally\n",
    "\n",
    "-----------------------\n",
    "\n",
    "The exercise wants you to train a simple model that classifies movie reviews as positive or negative using the text of the review. The model needs to be simple to run fast; in real-life, you would consider prompt-design or tuning of a pre-trained LLM.\n",
    "\n",
    "Here, you build a simple recurrent neural network (RNN) model in TensorFlow to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ns56O28f4bY",
    "tags": []
   },
   "source": [
    "#### Train the model\n",
    "\n",
    "You train an LSTM model in TensorFlow on the IMDB sentiment classification task.\n",
    "\n",
    "**Note**: *model training takes ~=5 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up deterministic results\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "weight_init = tf.keras.initializers.GlorotNormal(seed=42)\n",
    "bias_init = tf.keras.initializers.Zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tLJKt1uHJ8La"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 137s 172ms/step - loss: 0.4450 - accuracy: 0.7846 - val_loss: 0.3617 - val_accuracy: 0.8407\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 126s 161ms/step - loss: 0.2790 - accuracy: 0.8849 - val_loss: 0.3695 - val_accuracy: 0.8402\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.3695 - accuracy: 0.8402\n",
      "Loss: 0.3694600760936737  Accuracy: 0.8402400016784668\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, name=\"embeddings\"))\n",
    "model.add(LSTM(128, dropout=0.2, kernel_initializer=weight_init, bias_initializer=bias_init))\n",
    "model.add(Dense(1, activation=\"sigmoid\", kernel_initializer=weight_init, bias_initializer=bias_init))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=32)\n",
    "\n",
    "print(f\"Loss: {loss}  Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f9_ct3sc_AF"
   },
   "source": [
    "#### Export the model\n",
    "\n",
    "Next, you export the model to your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bxuGLTEpR7Dg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://explainable-ai-qwiklabs-gcp-01-e97fc0ced76b/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://explainable-ai-qwiklabs-gcp-01-e97fc0ced76b/model/assets\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = f\"{BUCKET_URI}/model\"\n",
    "\n",
    "tf.saved_model.save(model, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySldGR7eKdY0",
    "tags": []
   },
   "source": [
    "## Upload the custom-trained model for deployment\n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "In Vertex AI, models are uplaoded to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction) as a `Vertex AI Model` resource.\n",
    "\n",
    "You can configure and specify the feature-based explanations within the Model itself. Then, simply upload the model to Model Registry using the *aiplatform* library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnefcZK1ba4b"
   },
   "source": [
    "### Configure explanation settings\n",
    "\n",
    "To configure explanations, you need to define an explanation spec that includes:\n",
    "- [ExplanationParameters()](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.ExplanationParameters) to configure explaining for Model's prediction. \n",
    "- [ExplanationMetadata()](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.ExplanationMetadata) to describe the Model's input and output for explanations.\n",
    "\n",
    "Learn more at [configuring feature-based explanations](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-feature-based)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure explanation metadata\n",
    "\n",
    "You get the signatures of your model's input and output layers by reloading the model into memory, and querying it for the signatures corresponding to each layer. The input layer name of the serving function will be used later when you configure explanation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vVdSjoBSQaEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving function input: embeddings_input\n",
      "Serving function output: dense\n"
     ]
    }
   ],
   "source": [
    "# Get the model signature\n",
    "MODEL_DIR = f\"{BUCKET_URI}/model\"\n",
    "saved_model = tf.saved_model.load(MODEL_DIR)\n",
    "serving_input = list(\n",
    "    saved_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    ")[0]\n",
    "serving_output = list(\n",
    "    saved_model.signatures[\"serving_default\"].structured_outputs.keys()\n",
    ")[0]\n",
    "\n",
    "print(\"Serving function input:\", serving_input)\n",
    "print(\"Serving function output:\", serving_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metadata for explanations\n",
    "INPUT_METADATA = {\n",
    "    \"my_input\": aiplatform.explain.ExplanationMetadata.InputMetadata(\n",
    "        {\n",
    "            \"input_tensor_name\": serving_input,\n",
    "            \"encoding\": aiplatform.explain.ExplanationMetadata.InputMetadata.Encoding(\n",
    "                1\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "OUTPUT_METADATA = {\n",
    "    \"my_output\": aiplatform.explain.ExplanationMetadata.OutputMetadata(\n",
    "        {\"output_tensor_name\": serving_output}\n",
    "    )\n",
    "}\n",
    "\n",
    "metadata = aiplatform.explain.ExplanationMetadata(\n",
    "    inputs=INPUT_METADATA, outputs=OUTPUT_METADATA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure explanation parameters\n",
    "\n",
    "Explanation parameters are mutually exclusive which means you can select only one of the following for feature-based explanation for a Model:\n",
    "- `sampled_shapley_attribution`\n",
    "- `integrated_gradients_attribution`\n",
    "- `xrai_attribution`\n",
    "    \n",
    "In this lab, you use [sampled_shapley_attribution](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.SampledShapleyAttribution) with 10 feature permutations used when approximating the Shapley values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_wDTAJoJHx5p"
   },
   "outputs": [],
   "source": [
    "FEATURE_PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
    "feature_parameters = aiplatform.explain.ExplanationParameters(FEATURE_PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLslkKlneNyX"
   },
   "source": [
    "### Upload the model to `Model Registry`\n",
    "\n",
    "The explanation spec and the path to the trained model are used to upload the Model in `Model Registry`. Vertex AI provides [Docker container images](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers) with pre-installed packages that you run as pre-built containers for serving predictions and explanations from trained model artifacts.\n",
    "\n",
    "**Note:** Uploading the model to Model Registry can take a few minutes, usually 3-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQ_oIG11ID1o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model backing LRO: projects/208312552470/locations/us-central1/models/6456893029230837760/operations/2185939101541203968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/208312552470/locations/us-central1/models/6456893029230837760/operations/2185939101541203968\n"
     ]
    }
   ],
   "source": [
    "uploaded_feature_model = aiplatform.Model.upload(\n",
    "    display_name=\"tf_feature_expl\",\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\",\n",
    "    explanation_parameters=feature_parameters,\n",
    "    explanation_metadata=metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfL8qvYlLOo6",
    "tags": []
   },
   "source": [
    "## Deploy the model for online prediction\n",
    "\n",
    "Now that the Model has been created in Vertex AI Model Registry, you can deploy it as an endpoint.\n",
    "\n",
    "You can specify any [compute resources](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute) for your endpoint; here, you specify a small instance since it is a small model.  \n",
    "\n",
    "**Note:** Deploying the model to an endpoint can take a few minutes, usually 5-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_endpoint = uploaded_feature_model.deploy(\n",
    "    deployed_model_display_name=\"tf_feature_deploy\",\n",
    "    machine_type=\"e2-standard-2\" ,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Send an online prediction request with explanations\n",
    "\n",
    "----------------\n",
    "\n",
    "With the model endpoint created, you can now send an `explain` request. The request takes an encoded input text data and returns the predictions with explanations.\n",
    "\n",
    "As you request feature-based explanations on a text classification model, you get the predicted class along with an explanation, indicating how much each word or token contributes to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to send an online predition request for a test example at the specified `review` index,\n",
    "# and extract prediction with explanations\n",
    "\n",
    "def send_explanation_request(endpoint, review=0):\n",
    "    example = x_test[review].tolist()\n",
    "    example_label = y_test[review]\n",
    "    result = endpoint.explain([example]) # Ask Vertex AI to explain\n",
    "    \n",
    "    prediction = result.predictions[0][0] # Model's prediction\n",
    "    explanations = result.explanations[0] # Explanations\n",
    "    \n",
    "    return example, example_label, prediction, explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to decode the embedding into the original text sentence\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = {value: key for (key, value) in index.items()}\n",
    "\n",
    "def decode_sentence(x, index=reverse_index):\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([index.get(i - 3, \"UNK\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example, example_label, prediction, explanations = send_explanation_request(feature_endpoint, review=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†Extract relevant info from the feature-based explanations\n",
    "\n",
    "The feature-based explanations contain the following fields:\n",
    "- `baseline_output_value`: output value for a baseline instance.\n",
    "- `instance_output_value`: output value for the instance provided.\n",
    "- `feature_attributions`: a description of the contributions of each feature; in this example, the first value applies to the first word in the sentence, and so on.\n",
    "- `output_index`: index of the output.\n",
    "- `approximation_error`: if above 0.05, consider adjusting the explanation spec. \n",
    "- `output_name`: it corresponds to the serving_output, i.e. the name of the last layer of the model.\n",
    "\n",
    "You focus on the `feature_attributions`, and extract them as an array below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = explanations.attributions[0].feature_attributions[\"my_input\"]\n",
    "attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize and analyze the explanations\n",
    "\n",
    "\n",
    "------------\n",
    "\n",
    "For exploration and analysis, visualizations are often the most useful tool. You can visualize the attributions using matplotlib or other plotting tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTegf0H-1M3M",
    "tags": []
   },
   "source": [
    "#### Method 1: Matplotlib colormap\n",
    "\n",
    "You can visualize the attributions for the text instance by mapping the values of the attributions onto a matplotlib colormap.\n",
    "\n",
    "In the colormap, words are highlighted in the text following their attribution values. Words with high positive attribution are highlighted in shades of green and words with negative attribution in shades of pink. Stronger shading corresponds to higher attribution values. Positive attributions can be interpreted as increase in probability of the predicted class while negative attributions correspond to decrease in probability of the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ9b03pQ9W86"
   },
   "outputs": [],
   "source": [
    "def highlight(string, color=\"white\"):\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\"\n",
    "\n",
    "\n",
    "def colorize(attrs, cmap=\"PiYG\"):\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "\n",
    "    # now compute hex values of colors\n",
    "    colors = list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "    return colors\n",
    "\n",
    "\n",
    "def display_highlights(example, example_label, prediction, attributions):\n",
    "    words = decode_sentence(example).split()\n",
    "    colors = colorize(attributions)\n",
    "\n",
    "    print(f\"Prediction:[{prediction}] Actual:[{example_label}]\\n\")\n",
    "\n",
    "    display(HTML(\"\".join(list(map(highlight, words, colors)))))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SEieQI-9FIy"
   },
   "outputs": [],
   "source": [
    "words = display_highlights(example, example_label, prediction, attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Method 2: Matplotlib barh\n",
    "A more classic visualization of feature attributions is the horizontal bar plot.\n",
    "\n",
    "Let's use a different test instance, and start by displaying the text highlighted again to see what review we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SEieQI-9FIy"
   },
   "outputs": [],
   "source": [
    "# Use review 10 this time.\n",
    "example, example_label, prediction, explanations = send_explanation_request(feature_endpoint, review=10)\n",
    "attributions = explanations.attributions[0].feature_attributions[\"my_input\"]\n",
    "words = display_highlights(example, example_label, prediction, attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, display a Feature Attribution plot to view the magnitudes of sentiment contributions from the words in this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with words and feature attribution values \n",
    "importance_dict = {k:v for k, v in zip(words, attributions)}\n",
    "\n",
    "# Sort by feature attributions \n",
    "sorted_feature_names = sorted(importance_dict, key=importance_dict.get)\n",
    "\n",
    "# List sorted values \n",
    "sorted_attr_values = [importance_dict[key] for key in sorted_feature_names]\n",
    "num_features = len(sorted_feature_names)\n",
    "\n",
    "if num_features > 0:\n",
    "    x_pos = list(range(num_features))\n",
    "    plt.figure(figsize=(8,11))\n",
    "    plt.barh(x_pos, sorted_attr_values)\n",
    "    plt.yticks(x_pos, sorted_feature_names)\n",
    "    plt.title('Feature attributions')\n",
    "    plt.ylabel('Feature names')\n",
    "    plt.xlabel('Attribution value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the plots: what do the feature-based explanations say?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è BEWARE: it's a simple model with one LSTM layer (128 units) trained for 2 epochs ü§ñ**\n",
    "\n",
    "*We leave the analysis of the first review, visualized with Method 1, as an exercise.*\n",
    "\n",
    "Let's focus on the second review analyzed in the feature attribution plot with Method 2. \n",
    "<br>You can easily see that typically happy words end up being attributed to positive scores and typically negative words are attributed to negative scores. The words *\"good\"* and *\"guaranteed\"* have a strong positive influence while the words *\"chuckles\"* and *\"rid\"* have strong negative influence. The model learned these associations in bulk from the review words and labels in our training set. Vertex Explainable AI further allows you to see specifically what the model learned, which can help you find potentially problematic scores.\n",
    "\n",
    "For example, in this review, the word *\"black\"* appears in the phrase \"black comedy\", which is neither positive nor negative, just a type of comedy. The model, however, doesn't see *\"black\"* as a modifier to *\"comedy\"*. It is its own unique word. Across the rest of the dataset, *\"black\"* likely appeared in more negative reviews than positive reviews, which is why it got the negative attribution. Since this review is labeled and predicted as positive (typically showing predictions in the 90% range, depending on the random initialization undergone by your model), it likely mediated how negative *\"black\"* would have otherwise been. While this could be entirely coincidental, it could also point to biases in the data that you want to address. Further data exploration is definitely the next step!\n",
    "\n",
    "Thankfully, you have much more powerful language processing tools now with Generative AI! You can use not just single words, but the entire context to understand meaning. You could use [PaLM Text Embeddings](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/textembedding-gecko) to provide a vector representation of the entire review, rather than each word independently. You could then use a simpler dense (rather than recurrent) neural network to learn positive and negative scores from all of the context and nuance in the natural language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "xai_text_classification_feature_attributions.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": ".m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
